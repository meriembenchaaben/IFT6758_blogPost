---
layout: post
title: MileStone 2
---


## Task 2: Feature Engineering I

## Task 3: Baseline Models
### Question 1

## Task 4: Feature Engineering II

## Task 5: Advanced Models
# XGBoost models 


### Question 1


In order to enhance  the performance of our models of predicting if an event is a goal or not, we used the  train-validation split procedure. The validation data enables us to fine-tune the model hyperparameters and make decisions regarding what changements we can apply to have better result. The validation set affects indirectly the model. 
In our case, we are working with very few hyperparameters, thus the size of our dataset is not very huge (20%) of the training Data.  
Link to comet: 



### Question 2

Hyperparameter tuning setup: 

we are using the "binary:hinge"  loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.

The learning rate is controlled by the ETA parameter. It defines the amount of "correction" we perform at each phase by corresponding to the shrinking of the weights associated with features after each cycle. A smaller eta strengthens our model's resistance to overfitting, therefore the lower the learning rate, the better.


Tuning the Number of Decision Trees in XGBoost:
We crun a grid search of the n estimators model parameter with scikit-learn, assessing this sequence of values (50, 150, 200, 250, 300, 350,400)We note that the  default in the XGBoost library is 100 . In order to evvaluate the results of each configuraton we use F1 score. We obtain the best results 350. 


Results: 

- Links to experiments in *comet.ml*:
    - [Link to the experiment of dataset statistics in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/b65854538ffd4dddaf1378a9679e23f1)
    - [Link to experiments of Logistic Regression (distance) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/fbea37d0aa6d46a488c82cb6fe912fb0)
    - [Link to experiments of Logistic Regression (angle) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/a0a1675d0895408faf2f476f91cf6e89)

- Links to registered models:


### Question 3


We proceed first by styudying the correlation between the features so we plot the following plot: 
<figure>
<img src="/assets/milestone2/correlation.png" alt="">
<figcaption style="text-align:center;">Figure 1.1: Features correlation</figcaption>
</figure>
We notice that "gameseconds" and "period" features  are very correlated. LasteventGameSeconds and period too. 
Thanks to such results we were able to remove redudant features.
In the next experiments we should be reducing  this redudancy by neglecting the "period" feature.

We also notice that distanceFromTheNet is the feature the most correlated with our target feature "goal".
This feature should be present in all the coming experiments.

From the  previous task, we can extract a set of important features based on fitted trees and this using the predifined libray plotting method plot_importance, we obtain the following plot: 
<figure>
<img src="/assets/milestone2/FeatureImportance_XGboost_.png" alt="">
<figcaption style="text-align:center;">Figure 1.1: Features Importance regarding XGboost</figcaption>
</figure>

Another tool we used to check what features are actually enhancing the prediction accuracy; SHAP library. 
we obtain this plot: 
<figure>
<img src="/assets/milestone2/Shap.png" alt="">
<figcaption style="text-align:center;">Figure 1.1: SHAP features selected</figcaption>
</figure>
The idea then is to run an XGboost model with only the Features pushing the prediction higher (shown in red).
Results: 


A second option was to run Lasso: 
In this case only two features are suggested to be selected: ['distanceFromNet', 'speed']. We peroform then a second XGboost with the already tuned  parameters previously but this time using these two features and a feature related to previousEvent.  



Feature Selection using Wrapper methods: 
We thought about using Feature importance scores  for feature selection and this was done by using selectFromModel class already provided by sklearn. Athreshold is provided to this method to select a set of features, in our case (wrapper method) this threshold is obtained after first training  and then evaluating an XGBoost model on the entire training dataset and test datasets respectively.

| Classifier                                  | AUC of ROC | Comet link |
|---------------------------------------------|:----------:|:----------:|
| XGboost All features (distance)             | 0.685      |            |
| Logistic Regression (angle)                 | 0.507      |            |
| Logistic Regression (angle+distance)        | 0.685      |            |
| Random baseline with uniform distribution   | 0.5        |            |


## Task 6: Give it your best shot! 
### Overview
In this section we attempt several new ideas in order to achieve better accuracy, compared to previous sections.

In order to improve our models' accuracy _(and achieve full marks)_, in this section we explore the following additional approaches:

1. [x] Data Train/Validation split using a **Time-Series Split**
2. [x] **Hyperparameter Tuning** using **Cross Validation**
3. [x] **Regularization** of model weights, to improve generalization / avoid overfitting
4. [x] Additional **Feature Selection** Techniques
   - [x] Model Weight-Based Feature Selection, using Support Vector Machines
   - [x] Recursive Feature Elimination-based Feature Selection, using Naive Bayes, Random Forrest and XGBoost Models
5. [x] Additional Models
   - [x] **Random Forest**
   - [x] **Neural Network** Models, 
   - [x] with different Loss functions
     - [x] Cross Entropy Loss
     - [x] Focal Loss
   - [x] with different Learning Rate policies
     - [x] One-Cycle Policy
     - [x] Stochastic Gradient Descent with Warm Restarts (SGDR) Learning Rate Policy
6. [x] Additional **Accuracy Metrics**
   - [x] **F1 Score**, Macro average, to emphasize minority class predictive accuracy
   - [x] **Brier Score**, to measure accuracy of predicted probabilities 

### Summary of Results -- Table of Results

|Classifier                                          |AUC of ROC (+) |F1 (Macro Average) (+) |Brier Score (-) |Comet link                                                                  |
|----------------------------------------------------|----------|------------------|-----------|----------------------------------------------------------------------------|
|Random Forest                                       |0.748     |0.525             |**0.091**      |[Details](https://www.comet.ml/meriembchaaben/ift6758/561f6ad677da470986f66c916519e9cf)|
|Feature Selection + XGBoost + Regularization        |0.773     |0.614             |0.174      |[Details](https://www.comet.ml/meriembchaaben/ift6758/6b56e4f49b6740548f83d6a16c13dc6d)|
|**XGBoost + Regularization + Grid Search**              |**0.774**     |**0.612**             |0.173      |[Details](https://www.comet.ml/meriembchaaben/ift6758/16c4746d9f314fa9b0038911ee4cdfac)|
|Neural Network (cross entropy loss, early stopping) |0.538     |0.547             |0.092      |[Details](https://www.comet.ml/meriembchaaben/ift6758/3533c1ad47bf4d53b53edccf0de74f1e)|
|Neural Network (focal loss, gamma=2, early stopping)|0.546     |0.558             |0.106      |[Details](https://www.comet.ml/meriembchaaben/ift6758/dd864f6af8094e1ab1c19fbcb4115278)|
|Neural Network (focal loss, gamma=2, no early stopping)|0.56      |0.558             |0.159      |[Details](https://www.comet.ml/meriembchaaben/ift6758/9128c55a6c5e402ea8f1a6e660e89994)|


**Table:** Summary of results.

**Note**: (+) means higer is better, (-) means lower is better

### Summary of Results -- Our Best Model
Our best model (ROC AUC=0.774) was **XGBoost** `XGBClassifier` with :
1. No Explicit Feature Selection
2. `L1`, `L2`, and tree pruning **regularization**, 
3. **Hyperparameter Tuning** using **Cross Validation** (n=5) on a Time Series data split (`TimeSeriesSplit`) (i.e. using 5 equal-length, consecutive, non-overlappping validation sets).

Our preprocessing pipeline included:
1. Categorical Encoding, using `OrdinalEconder`
2. Missing Value Imputation (`median` for numerical, `"Missing Value"` category for categorical data)
3. Standardization, using `StandardScaler`.
To accomplish these steps we took advantage of the `sklearn` `Pipeline` functionality.
 
### Summary of Results -- All Our Models
The best performing family of models was the Tree-based family of models.

Extreme Gradient Boosted Trees dominated across all metrics with 0.774 ROC AUC, 0.612 F1 (macro), 0.173 Brier score and were closly followed by Random Forest models with 0.748 ROC AUC, 0.525 F1 (macro), 0.091 Brier Score.

Perhaps surprisingly, Neural-Network (NN) based models were not able to fit to the data as well as Tree-based models did. 
Our best NN model achieved 0.546 ROC AUC, 0.558 F1 (macro) and 0.106 Brier score, which is worse than our best XGBoost model, with the exception of Brier scores, where lower is better.


### Detaield Results -- Our Best Model

### Detailed Results -- All Our Models

![ROC Curve](../assets/milestone2/q6-roc.png)

![Goal Rate Percentile](../assets/milestone2/6-goal_rate_percentile_1.png)

![Goal Rate Percentile, Cumulative](../assets/milestone2/q6-goal_rate_percentile_2.png)

![Calibration Diagram](../assets/milestone2/6-calibration_diagram.png)

![Precision Recall Curve](../assets/milestone2/6-precision_recall_curve.png)


## Task 7: Evaluate on test set

