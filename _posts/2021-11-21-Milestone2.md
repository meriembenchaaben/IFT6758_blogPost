---
layout: post
title: MileStone 2
---


## Task 2: Feature Engineering I

## Task 3: Baseline Models
### Question 1

## Task 4: Feature Engineering II

## Task 5: Advanced Models
# XGBoost models 


### Question 1


In order to enhance  the performance of our models of predicting if an event is a goal or not, we used the  train-validation split procedure. The validation data enables us to fine-tune the model hyperparameters and make decisions regarding what changements we can apply to have better result. The validation set affects indirectly the model. 
In our case, we are working with very few hyperparameters, thus the size of our dataset is not very huge (20%) of the training Data.  
<br />
Link to comet: 



### Question 2

**Hyperparameter tuning setup:** 
<br />
we are using the "binary:hinge"  loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.
<br />

The **learning rate** is controlled by the ETA parameter. It defines the amount of "correction" we perform at each phase by corresponding to the shrinking of the weights associated with features after each cycle. A smaller eta strengthens our model's resistance to overfitting, therefore the lower the learning rate, the better.


**Tuning the Number of Decision Trees in XGBoost:**
<br />
We run a **grid search**  of the n estimators (number of decison trees) model parameter with scikit-learn, assessing this sequence of values (50, 150, 200, 250, 300, 350,400) We note that the  default in the XGBoost library is 100 . In order to evaluate the results of each configuraton we use F1 score. We obtain the best results 350. 


**Results:** <br />
After running each model, we study the accuracy and F1 score(combination of recall and precision). The confusion matrix should illustrate perfectly these metrics.

<figure>
<img src="/assets/milestone2/Confusion_Matrix_AllFeatures.png" alt="">
<figcaption style="text-align:center;">Figure 5.1:Confusion Matrix_XGboost Tuned, input:  all features</figcaption>
</figure>

- Links to experiments in *comet.ml*:
    - [Link to the experiment of dataset statistics in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/b65854538ffd4dddaf1378a9679e23f1)
    - [Link to experiments of Logistic Regression (distance) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/fbea37d0aa6d46a488c82cb6fe912fb0)
    - [Link to experiments of Logistic Regression (angle) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/a0a1675d0895408faf2f476f91cf6e89)

- Links to registered models:


### Question 3


We proceed first by styudying the **correlation between the features** so we plot the following plot: 
<figure>
<img src="/assets/milestone2/correlation.png" alt="">
<figcaption style="text-align:center;">Figure 5.1: Features correlation</figcaption>
</figure>
 <br />
We notice that gameseconds and period features  are very correlated. LasteventGameSeconds and period too. 
Thanks to such results we were able to remove redudant features.
In the next experiments we should be reducing  this redudancy by neglecting the **period** feature.

We also notice that **distanceFromTheNet** is the feature the most correlated with our target feature **goal**.
This feature should be present in all the coming experiments.
Some correlation values are explainable such as the speed and the distance from last Event (proportional).

From the  previous task, we can extract a set of important features based on fitted trees and this using the predifined libray plotting method plot_importance, we obtain the following plot: 
<figure>
<img src="/assets/milestone2/FeatureImportance_XGboost_.png" alt="">
<figcaption style="text-align:center;">Figure 5.1: Features Importance regarding XGboost</figcaption>
</figure>
 <br />
Another tool we used to check what features are actually enhancing the prediction accuracy; SHAP library. 
we obtain this plot: 
<figure>
<img src="/assets/milestone2/Shap.png" alt="">
<figcaption style="text-align:center;">Figure 5.1: SHAP features selected</figcaption>
</figure>


 <br />
The idea then is to run an XGboost model with only the Features pushing the prediction higher (shown in red) ['changeInAngleShot','lastEventXCoord','angle','distanceFromNet'].
Results: 
<figure>
<img src="/assets/milestone2/Confusion_Matrix_SelectedFeatures_SHAP.png" alt="">
<figcaption style="text-align:center;">Figure 5.1:Confusion Matrix_XGboost Tuned, input:  Selected  features by SHAP</figcaption>
</figure>
 <br />
A second option was to run **Lasso**:
 <br />

In this case only two features are suggested to be selected: ['distanceFromNet', 'speed']. We peroform then a second XGboost with the already tuned  parameters previously but this time using these two features and a feature related to previousEvent. 
We obtain these results. 
<figure>
<img src="/assets/milestone2/Confusion_Matrix_SelectedFeatures_Lasso.png" alt="">
<figcaption style="text-align:center;">Figure 5.1:Confusion Matrix_XGboost Tuned, input:  selected features by Lasso</figcaption>
</figure>
 <br />


**Feature Selection using Wrapper methods:** 
<br />
We thought about using Feature importance scores  for feature selection and this was done by using selectFromModel class already provided by sklearn. Athreshold is provided to this method to select a set of features, in our case (wrapper method) this threshold is obtained after first training  and then evaluating an XGBoost model on the entire training dataset and test datasets respectively.

| Classifier                                  | AUC of ROC  |brier_score_loss|
|---------------------------------------------|:----------:|:----------:|
| XGboost (distanceFromNet & angle)           | 0.5014     | 0.0934     |
| XGboost All features                        | 0.5026     | 0.0935     |  
| XGboost SHAP features                       |0.5017      |  0.0948    |
| XGboost Lasso features (angle+distance)     |0.5045      | 0.0950     | 
| Random baseline with uniform distribution   | 0.5        |            |       


## Task 6: Give it your best shot! 

## Task 7: Evaluate on test set

