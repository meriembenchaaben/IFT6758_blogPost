---
layout: post
title: MileStone 2
---


## Task 2: Feature Engineering I

### Question 1

Figure 2.1 depicts the histogram of shot counts (goals and no-goals separated), binned by distance.

<figure> 
<img src="/assets/milestone2/2-shots-per-distance.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 2.1: Histogram of shot counts (goals and no-goals separated), binned by distance</figcaption>
</figure>

We clearly notice high count of shots and goals for lower distances.
Players tend to shot/score from distances close to the net and it is rare to see shots/goals from the defense zone of the team to the net of the opposing team.
Also, goals are more probable for lower distances close to the net.


Figure 2.2 shows the histogram of shot counts (goals and no-goals separated), binned by angle.

<figure> 
<img src="/assets/milestone2/2-shots-per-angle.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 2.2: Histogram of shot counts (goals and no-goals separated), binned by angle</figcaption>
</figure>

We see from that figure that players tend to shot and score when they are straight ahead the net (90°) and it is less frequent to score from lower or higher angles since it is difficult to score in these cases when we are not straight ahead the net.



Figure 2.3 depicts two plots that represent 2D histogram of shot counts (goals and no-goals separated) binned by distance and angle.

<figure> 
<img src="/assets/milestone2/2-shots-per-distance-angle-1.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<img src="/assets/milestone2/2-shots-per-distance-angle-2.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<figcaption style="text-align:center;">Figure 2.3:  2D histogram of shot counts (goals and no-goals separated) binned by distance and angle</figcaption>
</figure>

It is more often to shot when players are straight ahead and close to the net.
It is rare to shot from higher distances or from behind the net.


### Question 2

Figure 2.4 shows the goal rate per distance.

<figure> 
<img src="/assets/milestone2/2-goals-rate-distance-1.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<img src="/assets/milestone2/2-goals-rate-distance-2.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<figcaption style="text-align:center;">Figure 2.4: Goal rate per distance</figcaption>
</figure>

This figure confirms previous observations.
We have higher goal rates for lower distances.
Players have more success rate when they are close to the net.

Figure 2.5 shows the goal rate per angle.

<figure> 
<img src="/assets/milestone2/2-goals-rate-angle-1.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<img src="/assets/milestone2/2-goals-rate-angle-2.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<figcaption style="text-align:center;">Figure 2.5: Goal rate per angle</figcaption>
</figure>

This figure affirms our previous findings.
We have high goal rate when they are straight ahead the net (90°) and it is more difficult to score from bigger and lower angles.


### Question 3

Figure 2.6 shows the count of goals binned by distance for empty and non-empty net events.

<figure> 
<img src="/assets/milestone2/2-goals-distance-net.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 2.6: Goals count binned by distance for empty and non-empty net events</figcaption>
</figure>

Goal count is high for low distances and it more difficult to score from high distances especially from defensive zone.
In real world, it is incredibly rare to score a non-empty net goal from whithin the defensive zone.

In order to detect an event with incorrect features, we sorted the goals with distance (decreasing order) and we filtered the events to keep non-empty net goals.
Then, we selected the events in the top of that list (i.e. non-empty net goals with highest distances). 

We identified these goal events and watched them online in the internet (NHL gamecenter and Youtube) and checked if the features of these goal events are correct or not.

Most of these identified events had incorrect features according to the videos that we have seen.

On of the identified events is shown in Figure 2.6.

<figure> 
<img src="/assets/milestone2/2-anomaly.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 2.6: Example of event with incorrect (x,y) features</figcaption>
</figure>

According to the NHL API, this goal was scored from *(-97, 21)* while the net is non-empty and the team of the shooter is on the left rink side. Thus, we conclude that this goal was scored from an approximate distance of *187 ft* (see Figure 2.6).

These are links to the video of this goal:

 - Youtube: [https://youtu.be/uddEunFRpko?t=145](https://youtu.be/uddEunFRpko?t=145) 
 - NHL center: [https://www.nhl.com/video/mackenzie-buries-the-one-timer/t-277350912/c-47688603](https://www.nhl.com/video/mackenzie-buries-the-one-timer/t-277350912/c-47688603)

When watching the goal video, we noticed that the *(x, y)* coordinates where incorrect.
The goal was scored from a closer distance to the net.









## Task 3: Baseline Models

Apart from the test set, we splitted the rest of the data, resulted from **Feature Engineering I**, into train and validation sets: *20%* validation set and *80%* training set.
Table 3.1 describes the size of the different dataset splits.

| Dataset size        | 311086 |
| Training set size   | 248868 |
| Validation set size | 62218  |

<dev style="display:block;text-align:center;">Table 3.1: Dataset information </dev>

### Question 1
We trained a *Logistic Regression* classifier on the distance feature only, and we evaluated its accuracy (i.e., correctly predicted / total) on the validation set. 
The accuracy of our model during training is **0.9063** and its accuracy on the validation set is **0.9058**.
We looked at the predictions of our model and we noticed that it is always predicting **0**. this means that the model is always predicting a shot (i.e., non-goal ) for all the observations.
To investigate the potential reasons behind this issue, we computed the goal rate in the dataset given by the following formula:

> Goal rate = Count of goals / Count of shots and goals

Equivalently,
> Non-Goal rate = Count of shots / Count of shots and goals    

where: *Non-goals = Shots*

Table 3.2 depicts the goal rate in the different datasets.

| Dataset        | Goal rate | Non-Goal rate |
|----------------|------------|----------------|
| ALL dataset    | 0.0938     | 0.9062         |
| Training set   | 0.0937     | 0.9063         |
| Validation set | 0.0942     | 0.9058         |

<dev style="display:block;text-align:center;">Table 3.2: Goal/non-goal rates for the different datasets </dev>

This table helps us figuring out the potential issue.
In fact, the dataset is imbalanced. We have more than **90%** of the data is shots (i.e. non-goals).
So, the model is always predicting *non-goal* for all the events and we obtain **90%** as accuracy which is not relevant and not representative of the performance of our model since we are interested in expected goals (i.e. the probanility that a shot is a goal) and we aim that our model predicts well if an event could result in a goal.
Currently, we have the *accuracy* equal to the *non-goal rate* since our model is always predicting 0.
However, to have a fair and more significant evaluation, we should explore further the performance of our model using other metrics and plots.


### Question 2 & 3

We used the validation dataset for the plots since it allows to evaluate more fairly the model performance.

Figure 3.1 depicts the ROC curves for the different classifiers and Table 3.3 illustrates the AUC metric of ROC for these models.

<figure> 
<img src="/assets/milestone2/3-roc.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 3.1: ROC (Receiver Operating Characteristic) curve of the different classifiers</figcaption>
</figure>

| Classifier                                  | AUC of ROC |
|---------------------------------------------|:----------:|
| Logistic Regression (distance)              | 0.685      |
| Logistic Regression (angle)                 | 0.507      |
| Logistic Regression (angle+distance)        | 0.685      |
| Random baseline with uniform distribution   | 0.5        |

The ROC curve shows the diagnostic ability of binary classifiers. It depicts the true positive rate (i.e. sensitivity) against the false positive rate (i.e. 1 - specificity).
The curves of *LR(distance)* and *LR(distance+angle)* are identical which means that the *LR* classifier relies more on the *distance* feature and considers it as a more important feature as compared to the *angle* feature. 
This actually makes sense in terms of ROC curve and AUC metric as we clearly see that the *LR(angle)* classifier has worse *AUC* score and its *ROC* curve is below the other curves which means it is performing worse in predicting expected goals.
Compared to the *Random baseline*, the *LR(angle)* classifier has a comparable value 
For some threshold values, the ROC curve of *LR(angle)* is below that of the random classifier.
Eventhough *LR(distance)* and *LR(distance+angle)* have the *ROC* curve above that of the random baseline, their performance is not good enough as they have poor value of *AUC* (far from 1) which means poor discrimination between the two target classes.

*ROC* curves are also useful to determine a *cutoff value* for predicting expected goals.
This would definitely help improving the performance of the classifiers compared to the case where we use *0.5* as a *cutoff* value. 
This would improve the predictions of expected goals using our models.
We tried using the optimal cutoff point that maximizes the difference *True Positive Rate - False Positive Rate)*.
This allowed us to solve the problem of predicting all the events as non-goals and gave better predictions.


Table 3.5 shows the optimal cutoff points obtained from the ROC curve (code available in `notebooks/3-BaselineModels.ipynb`).
This threshold is obtained based on the maximization of the difference **True Positive Rate - False Positive Rate)**.


| Classifier                                  | Optimal cutoff point |
|---------------------------------------------|:--------------------:|
| Logistic Regression (distance)              | 0.09618              |
| Logistic Regression (angle)                 | 0.09333              |
| Logistic Regression (angle+distance)        | 0.09634              |

<dev style="display:block;text-align:center;">Table 3.5:  Best cutoff point for the different LR models</dev>





Figure 3.2 shows the goal rate by shot probability model percentile.

<figure> 
<img src="/assets/milestone2/3-goal_rate_percentile_1.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 3.2: Goal rate by shot probability model percentile</figcaption>
</figure>

Since, LR models are predicting low probability values (i.e. LR models are always predicting shots), we just see low values of percentiles.
For instance the 90th percentile is equal to *0.19* approximately.
We clearly see that the goal rate is increasing for higher shot probability model percentiles.
This gives us intuition to fine-tune the probability threshold for predicting goals and non-goals (i.e. cutoff point).
In fact, *0.5* does not seem a good value in that case for all the models.
We used the thresholds produced by the *ROC curve* to optimize this cutoff point for all the models as mentioned previously.
See notebook for *Task 3* for more details about the implementation.
As a result, *0.1* seems to be a good cutoff point for all the LR models.
It leads to better models performance.
 


Figure 3.3 shows the cumulative proportion of goals by shot probability model percentile.

<figure> 
<img src="/assets/milestone2/3-goal_rate_percentile_2.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 3.3: Cumulative proportion of goals by shot probability model percentile</figcaption>
</figure>

Similarly, we notice low values of shot probability model percentiles since LR models are predicting low probability values (i.e. LR models are always predicting shots).
The cumulative propotion of goals increases significantly for probabilities higher than *0.1*.
This confirms our choice of *cutoff point*.

Figure 3.4 depicts the reliability diagram (calibration curve) of the different models.

<figure> 
<img src="/assets/milestone2/3-calibration_diagram.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 3.4: Reliability diagram (calibration curve)</figcaption>
</figure>

The calibration diagram for the different LR models result in a single point.
It is clear that the probabilistic predictions of the different classifiers are not well calibrated since we just see low probability values in a low range (the range of probabilities is approximately [0,2]).
Thus we conclude that the calibration of the different basic models is poor.



### Question 4

- Links to experiments in *comet.ml*:
    - [Link to the experiment of dataset statistics in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/da7f146cc6974f08ae55f364017f04e1?experiment-tab=chart)
    - [Link to experiments of Logistic Regression (distance) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/72c0b5895c2a4e5bb311400e64868f5c)
    - [Link to experiments of Logistic Regression (angle) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/406b588706924ead920acfdc2bd6a2db)
    - [Link to experiments of Logistic Regression (distance+angle) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/8a9f4662828a41db89fc3258b926c7da)

- Links to registered models
    - [Link to Logistic Regression (distance) model in comet.ml ](https://www.comet.ml/meriembchaaben/model-registry/lr-distance-model)
    - [Link to Logistic Regression (angle) model in comet.ml ](https://www.comet.ml/meriembchaaben/model-registry/lr-angle-model)
    - [Link to Logistic Regression (distance+angle) model in comet.ml ](https://www.comet.ml/meriembchaaben/model-registry/lr-distance-angle-model)
    




## Task 4: Feature Engineering II

Here, we describe the features that our dataset includes.

- **lastEventType:** Type of previous event.
- **lastEventPeriod:** Period of previous event.
- **lastEventPeriodTime:** Time since period of previous event started. 
- **lastEventXCoord:** x-coordinate of previous event. 
- **lastEventYCoord:** y-coordinate of previous event. 
- **gameSeconds:** Playing time elapsed between the beginning of the game and the current event in seconds.
- **lastEventGameSeconds:** Playing time elapsed between the beginning of the game and the previous event in seconds.
- **timeFromLastEvent:** time between current event and last event in seconds.
- **distanceFromLastEvent:** Euclidean distance between current event and last event (if coordinates of both events are available).
- **rebound:** True if last event was a shot.  Otherwise it’s False.
- **lastEventAngle:** angle between goal and position of last event. 
- **changeInAngleShot:** (only if previous event was shot) change in angle between current and previous event. 
- **speed:** distanceFromLastEvent divided by timeFromLastEvent
- **timeSincePowerPlayStarted:** Time since power play of the event’s team started in seconds. 
- **numFriendlyNonGoalieSkaters:** number of friendly non-goalie players in the ice.
- **numOpposingNonGoalieSkaters:** number of opposing non-goalie players in the ice. 
- **strength2:** strength of team at current event (even, power play, short handed).

This is the [link to the filtered dataframe](https://www.comet.ml/meriembchaaben/ift6758/0255c5bf62c6425aa6147c4f317f3f28?experiment-tab=assets) in *comet.ml*.


## Task 5: Advanced Models

## Task 6: Give it your best shot! 

## Task 7: Evaluate on test set

