---
layout: post
title: MileStone 2
---


## Task 2: Feature Engineering I

### Question 1

Figure 2.1 depicts the histogram of shot counts (goals and no-goals separated), binned by distance.

<figure> 
<img src="/assets/milestone2/2-shots-per-distance.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 2.1: Histogram of shot counts (goals and no-goals separated), binned by distance</figcaption>
</figure>

We clearly notice high count of shots and goals for lower distances.
Players tend to shot/score from distances close to the net and it is rare to see shots/goals from the defense zone of the team to the net of the opposing team.
Also, goals are more probable for lower distances close to the net.


Figure 2.2 shows the histogram of shot counts (goals and no-goals separated), binned by angle.

<figure> 
<img src="/assets/milestone2/2-shots-per-angle.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 2.2: Histogram of shot counts (goals and no-goals separated), binned by angle</figcaption>
</figure>

We see from that figure that players tend to shot and score when they are straight ahead the net (90°) and it is less frequent to score from lower or higher angles since it is difficult to score in these cases when we are not straight ahead the net.



Figure 2.3 depicts two plots that represent 2D histogram of shot counts (goals and no-goals separated) binned by distance and angle.

<figure> 
<img src="/assets/milestone2/2-shots-per-distance-angle-1.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<img src="/assets/milestone2/2-shots-per-distance-angle-2.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<figcaption style="text-align:center;">Figure 2.3:  2D histogram of shot counts (goals and no-goals separated) binned by distance and angle</figcaption>
</figure>

It is more often to shot when players are straight ahead and close to the net.
It is rare to shot from higher distances or from behind the net.


### Question 2

Figure 2.4 shows the goal rate per distance.

<figure> 
<img src="/assets/milestone2/2-goals-rate-distance-1.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<img src="/assets/milestone2/2-goals-rate-distance-2.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<figcaption style="text-align:center;">Figure 2.4: Goal rate per distance</figcaption>
</figure>

This figure confirms previous observations.
We have higher goal rates for lower distances.
Players have more success rate when they are close to the net.

Figure 2.5 shows the goal rate per angle.

<figure> 
<img src="/assets/milestone2/2-goals-rate-angle-1.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<img src="/assets/milestone2/2-goals-rate-angle-2.png" alt="" style="margin:auto;float:right; width:50%; padding:5px;">
<figcaption style="text-align:center;">Figure 2.5: Goal rate per angle</figcaption>
</figure>

This figure affirms our previous findings.
We have high goal rate when they are straight ahead the net (90°) and it is more difficult to score from bigger and lower angles.


### Question 3

Figure 2.6 shows the count of goals binned by distance for empty and non-empty net events.

<figure> 
<img src="/assets/milestone2/2-goals-distance-net.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 2.6: Goals count binned by distance for empty and non-empty net events</figcaption>
</figure>

Goal count is high for low distances and it more difficult to score from high distances especially from defensive zone.
In real world, it is incredibly rare to score a non-empty net goal from whithin the defensive zone.

In order to detect an event with incorrect features, we sorted the goals with distance (decreasing order) and we filtered the events to keep non-empty net goals.
Then, we selected the events in the top of that list (i.e. non-empty net goals with highest distances). 

We identified these goal events and watched them online in the internet (NHL gamecenter and Youtube) and checked if the features of these goal events are correct or not.

Most of these identified events had incorrect features according to the videos that we have seen.

On of the identified events is shown in Figure 2.6.

<figure> 
<img src="/assets/milestone2/2-anomaly.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 2.6: Example of event with incorrect (x,y) features</figcaption>
</figure>

According to the NHL API, this goal was scored from *(-97, 21)* while the net is non-empty and the team of the shooter is on the left rink side. Thus, we conclude that this goal was scored from an approximate distance of *187 ft* (see Figure 2.6).

These are links to the video of this goal:

 - Youtube: [https://youtu.be/uddEunFRpko?t=145](https://youtu.be/uddEunFRpko?t=145) 
 - NHL center: [https://www.nhl.com/video/mackenzie-buries-the-one-timer/t-277350912/c-47688603](https://www.nhl.com/video/mackenzie-buries-the-one-timer/t-277350912/c-47688603)

When watching the goal video, we noticed that the *(x, y)* coordinates where incorrect.
The goal was scored from a closer distance to the net.









## Task 3: Baseline Models

Apart from the test set, we splitted the rest of the data, resulted from **Feature Engineering I**, into train and validation sets: *20%* validation set and *80%* training set.
Table 3.1 describes the size of the different dataset splits.

| Dataset size        | 311086 |
| Training set size   | 248868 |
| Validation set size | 62218  |

<dev style="display:block;text-align:center;">Table 3.1: Dataset information </dev>

### Question 1
We trained a *Logistic Regression* classifier on the distance feature only, and we evaluated its accuracy (i.e., correctly predicted / total) on the validation set. 
The accuracy of our model during training is **0.9063** and its accuracy on the validation set is **0.9058**.
We looked at the predictions of our model and we noticed that it is always predicting **0**. this means that the model is always predicting a shot (i.e., non-goal ) for all the observations.
To investigate the potential reasons behind this issue, we computed the goal rate in the dataset given by the following formula:

> Goal rate = Count of goals / Count of shots and goals

Equivalently,
> Non-Goal rate = Count of shots / Count of shots and goals    

where: *Non-goals = Shots*

Table 3.2 depicts the goal rate in the different datasets.

| Dataset        | Goal rate | Non-Goal rate   |
|----------------|------------|----------------|
| ALL dataset    | 0.0938     | 0.9062         |
| Training set   | 0.0937     | 0.9063         |
| Validation set | 0.0942     | 0.9058         |

<dev style="display:block;text-align:center;">Table 3.2: Goal/non-goal rates for the different datasets </dev>

This table helps us figuring out the potential issue.
In fact, the dataset is imbalanced. We have more than **90%** of the data is shots (i.e. non-goals).
So, the model is always predicting *non-goal* for all the events and we obtain **90%** as accuracy which is not relevant and not representative of the performance of our model since we are interested in expected goals (i.e. the probanility that a shot is a goal) and we aim that our model predicts well if an event could result in a goal.
Currently, we have the *accuracy* equal to the *non-goal rate* since our model is always predicting 0.
However, to have a fair and more significant evaluation, we should explore further the performance of our model using other metrics and plots.


### Question 2 & 3

We used the validation dataset for the plots since it allows to evaluate more fairly the model performance.

Figure 3.1 depicts the ROC curves for the different classifiers and Table 3.3 illustrates the AUC metric of ROC for these models.

<figure> 
<img src="/assets/milestone2/3-roc.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 3.1: ROC (Receiver Operating Characteristic) curve of the different classifiers</figcaption>
</figure>

| Classifier                                  | AUC of ROC |
|---------------------------------------------|:----------:|
| Logistic Regression (distance)              | 0.685      |
| Logistic Regression (angle)                 | 0.507      |
| Logistic Regression (angle+distance)        | 0.685      |
| Random baseline with uniform distribution   | 0.5        |

The ROC curve shows the diagnostic ability of binary classifiers. It depicts the true positive rate (i.e. sensitivity) against the false positive rate (i.e. 1 - specificity).
The curves of *LR(distance)* and *LR(distance+angle)* are identical which means that the *LR* classifier relies more on the *distance* feature and considers it as a more important feature as compared to the *angle* feature. 
This actually makes sense in terms of ROC curve and AUC metric as we clearly see that the *LR(angle)* classifier has worse *AUC* score and its *ROC* curve is below the other curves which means it is performing worse in predicting expected goals.
Compared to the *Random baseline*, the *LR(angle)* classifier has a comparable value 
For some threshold values, the ROC curve of *LR(angle)* is below that of the random classifier.
Eventhough *LR(distance)* and *LR(distance+angle)* have the *ROC* curve above that of the random baseline, their performance is not good enough as they have poor value of *AUC* (far from 1) which means poor discrimination between the two target classes.

*ROC* curves are also useful to determine a *cutoff value* for predicting expected goals.
This would definitely help improving the performance of the classifiers compared to the case where we use *0.5* as a *cutoff* value. 
This would improve the predictions of expected goals using our models.
We tried using the optimal cutoff point that maximizes the difference *True Positive Rate - False Positive Rate)*.
This allowed us to solve the problem of predicting all the events as non-goals and gave better predictions.


Table 3.5 shows the optimal cutoff points obtained from the ROC curve (code available in `notebooks/3-BaselineModels.ipynb`).
This threshold is obtained based on the maximization of the difference **True Positive Rate - False Positive Rate)**.


| Classifier                                  | Optimal cutoff point |
|---------------------------------------------|:--------------------:|
| Logistic Regression (distance)              | 0.09618              |
| Logistic Regression (angle)                 | 0.09333              |
| Logistic Regression (angle+distance)        | 0.09634              |

<dev style="display:block;text-align:center;">Table 3.5:  Best cutoff point for the different LR models</dev>





Figure 3.2 shows the goal rate by shot probability model percentile.

<figure> 
<img src="/assets/milestone2/3-goal_rate_percentile_1.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 3.2: Goal rate by shot probability model percentile</figcaption>
</figure>

Since, LR models are predicting low probability values (i.e. LR models are always predicting shots), we just see low values of percentiles.
For instance the 90th percentile is equal to *0.19* approximately.
We clearly see that the goal rate is increasing for higher shot probability model percentiles.
This gives us intuition to fine-tune the probability threshold for predicting goals and non-goals (i.e. cutoff point).
In fact, *0.5* does not seem a good value in that case for all the models.
We used the thresholds produced by the *ROC curve* to optimize this cutoff point for all the models as mentioned previously.
See notebook for *Task 3* for more details about the implementation.
As a result, *0.1* seems to be a good cutoff point for all the LR models.
It leads to better models performance.
 


Figure 3.3 shows the cumulative proportion of goals by shot probability model percentile.

<figure> 
<img src="/assets/milestone2/3-goal_rate_percentile_2.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 3.3: Cumulative proportion of goals by shot probability model percentile</figcaption>
</figure>

Similarly, we notice low values of shot probability model percentiles since LR models are predicting low probability values (i.e. LR models are always predicting shots).
The cumulative propotion of goals increases significantly for probabilities higher than *0.1*.
This confirms our choice of *cutoff point*.

Figure 3.4 depicts the reliability diagram (calibration curve) of the different models.
This plot allows to see how close is our predicted probability of a goal, to the frequency of goals.

<figure> 
<img src="/assets/milestone2/3-calibration_diagram.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 3.4: Reliability diagram (calibration curve)</figcaption>
</figure>

The calibration diagram for the different LR models results in a single point.
It is clear that the probabilistic predictions of the different classifiers are not well calibrated since we just see low probability values in a low range (the range of probabilities is approximately [0,2]).
Thus we conclude that the calibration of the different basic models is poor.



We conclude that LR are not performing well in the prediction of expected goals.
Thus, we should explore additional models to have a better performance.




### Question 4

- Links to experiments in *comet.ml*:
    - [Link to the experiment of dataset statistics in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/0c106d4c773846e8801a3dfba2db83fd)
    - [Link to experiments of Logistic Regression (distance) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/0107aaf3b32349bb856443ab83f7b584)
    - [Link to experiments of Logistic Regression (angle) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/d4a95f7ca52f43159f24d7ea8a00aeae)
    - [Link to experiments of Logistic Regression (distance+angle) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/6555b791b64c4dd79a9e500eef59873d)

- Links to registered models
    - [Link to Logistic Regression (distance) model in comet.ml ](https://www.comet.ml/meriembchaaben/model-registry/lr-distance-model)
    - [Link to Logistic Regression (angle) model in comet.ml ](https://www.comet.ml/meriembchaaben/model-registry/lr-angle-model)
    - [Link to Logistic Regression (distance+angle) model in comet.ml ](https://www.comet.ml/meriembchaaben/model-registry/lr-distance-angle-model)
    




## Task 4: Feature Engineering II

Here, we describe the features that our dataset includes.

- **lastEventType:** Type of previous event.
- **lastEventPeriod:** Period of previous event.
- **lastEventPeriodTime:** Time since period of previous event started. 
- **lastEventXCoord:** x-coordinate of previous event. 
- **lastEventYCoord:** y-coordinate of previous event. 
- **gameSeconds:** Playing time elapsed between the beginning of the game and the current event in seconds.
- **lastEventGameSeconds:** Playing time elapsed between the beginning of the game and the previous event in seconds.
- **timeFromLastEvent:** time between current event and last event in seconds.
- **distanceFromLastEvent:** Euclidean distance between current event and last event (if coordinates of both events are available).
- **rebound:** True if last event was a shot.  Otherwise it’s False.
- **lastEventAngle:** angle between goal and position of last event. 
- **changeInAngleShot:** (only if previous event was shot) change in angle between current and previous event. 
- **speed:** distanceFromLastEvent divided by timeFromLastEvent
- **timeSincePowerPlayStarted:** Time since power play of the event’s team started in seconds. 
- **numFriendlyNonGoalieSkaters:** number of friendly non-goalie players in the ice.
- **numOpposingNonGoalieSkaters:** number of opposing non-goalie players in the ice. 
- **strength2:** strength of team at current event (even, power play, short handed).

This is the [link to the filtered dataframe](https://www.comet.ml/meriembchaaben/ift6758/0255c5bf62c6425aa6147c4f317f3f28?experiment-tab=assets) in *comet.ml*.


## Task 5: Advanced Models
# XGBoost models 
### Overview
In this section we aim to select features  in order to  remove redundancy and identify the relevant features thus, to achieve better accuracy. We will also go through Xgboost parameters tuining. 
In the next section we will outline the methods and techinques we've attempted,  and we will finish with a brief discussion.

### Question 1


In order to enhance  the performance of our models of predicting if an event is a goal or not, we used the  train-validation split procedure. The validation data enables us to fine-tune the model hyperparameters and make decisions regarding what changements we can apply to have better result. The validation set affects indirectly the model. 
In our case, we are working with very few hyperparameters, thus the size of our dataset is not very huge (20%) of the training Data.  
<br />
The first experiment is to run an xgboost with only one feature"Distance": 
- Links to experiment in *comet.ml*:
    - [Link to the experiment of running XGboost with only distance feature in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/a5049227124c47f79e1c9e8426bf4aef)

The results are discussed later with regard to the rest of the models. The plots too are shown at the end of this Task. 

### Question 2

**Hyperparameter tuning setup:** 
<br />
we are using the "binary:hinge"  loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.
<br />

The **learning rate** is controlled by the ETA parameter. It defines the amount of "correction" we perform at each phase by corresponding to the shrinking of the weights associated with features after each cycle. A smaller eta strengthens our model's resistance to overfitting, therefore the lower the learning rate, the better.


**Tuning the Number of Decision Trees in XGBoost:**
<br />
We run a **grid search**  of the n estimators (number of decison trees) model parameter with scikit-learn, assessing this sequence of values (50, 150, 200, 250, 300, 350,400) We note that the  default in the XGBoost library is 100 . In order to evaluate the results of each configuraton we use F1 score, since we are faced to imbalanced class distribution to evaluate our model on. We obtain the best results 350. 

- Links to experiment in *comet.ml*:
    - [Link to the experiment of running grid search to finetune XGboost model in  comet.ml](https://www.comet.ml/meriembchaaben/ift6758/a5049227124c47f79e1c9e8426bf4aef)


**Results:** <br />
After running each model, we study the accuracy and F1 score(combination of recall and precision) after running the fine tuned model. The confusion matrix should illustrate perfectly these metrics.

<figure>
<img src="/assets/milestone2/Confusion_Matrix_AllFeatures.png" alt="">
<figcaption style="text-align:center;">Figure 5.1:Confusion Matrix_XGboost Tuned, input:  all features</figcaption>
</figure>

- Links to experiments in *comet.ml*:
    - [Link to the experiment of running the fine tuned XGboost with All features  in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/416d2cdda8754f3e8b07b38b225bc5b8)


- Links to registered models:


### Question 3


We proceed first by styudying the **correlation between the features** so we plot the following plot: 
<figure>
<img src="/assets/milestone2/correlation.png" alt="">
<figcaption style="text-align:center;">Figure 5.1: Features correlation</figcaption>
</figure>
 <br />
We notice that gameseconds and period features  are very correlated. LasteventGameSeconds and period too. 
Thanks to such results we were able to remove redudant features.
In the next experiments we should be reducing  this redudancy by neglecting the **period** feature.

We also notice that **distanceFromTheNet** is the feature the most correlated with our target feature **goal**.
This feature should be present in all the coming experiments.
Some correlation values are explainable such as the speed and the distance from last Event (proportional).

From the  previous task, we can extract a set of important features based on fitted trees and this using the predifined libray plotting method plot_importance, we obtain the following plot: 
<figure>
<img src="/assets/milestone2/FeatureImportance_XGboost_.png" alt="">
<figcaption style="text-align:center;">Figure 5.1: Features Importance regarding XGboost</figcaption>
</figure>
 <br />
Another tool we used to check what features are actually enhancing the prediction accuracy; SHAP library. 
we obtain this plot: 
<figure>
<img src="/assets/milestone2/Shap.png" alt="">
<figcaption style="text-align:center;">Figure 5.1: SHAP features selected</figcaption>
</figure>


 <br />
The idea then is to run an XGboost model with only the Features pushing the prediction higher (shown in red) ['changeInAngleShot','lastEventXCoord','angle','distanceFromNet'].
Results: 
<figure>
<img src="/assets/milestone2/Confusion_Matrix_SelectedFeatures_SHAP.png" alt="">
<figcaption style="text-align:center;">Figure 5.1:Confusion Matrix_XGboost Tuned, input:  Selected  features by SHAP</figcaption>
</figure>
 <br />
- Links to experiments in *comet.ml*:
    - [Link to the experiment of running the fine tuned XGboost with only features suggested by SHAP library in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/c3c4b0b443c64819812c8f6bff809349)
https://www.comet.ml/meriembchaaben/ift6758/392907348a514dcc9d2e6696cc160ac3
 
A second option was to run **Lasso**:
 <br />

In this case only two features are suggested to be selected: ['distanceFromNet', 'speed']. We peroform then a second XGboost with the already tuned  parameters previously but this time using these two features and a feature related to previousEvent. 
We obtain these results. 
<figure>
<img src="/assets/milestone2/Confusion_Matrix_SelectedFeaturesLasso.png" alt="">
<figcaption style="text-align:center;">Figure 5.1:Confusion Matrix_XGboost Tuned, input:  selected features by Lasso</figcaption>
</figure>
 <br />
- Links to experiments in *comet.ml*:
    - [Link to the experiment of running the fine tuned XGboost with only features suggested by LASSO method in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/392907348a514dcc9d2e6696cc160ac3)


**Feature Selection using Wrapper methods:** 
<br />
We thought about using Feature importance scores  for feature selection and this was done by using selectFromModel class already provided by sklearn. Athreshold is provided to this method to select a set of features, in our case (wrapper method) this threshold is obtained after first training  and then evaluating an XGBoost model on the entire training dataset and test datasets respectively. 
After observing  the results of this method we didn't consider it in the discussion since no emprovement was noted.  

**Summary of the considered Models:** 
We present in this table the different values for several metrics used to evaluate the implemented models: 


| Classifier                                  | AUC of ROC  |brier_score_loss|Accuracy|F1_score|
|---------------------------------------------|:----------:|:----------:|:----------:|:----------:|
| XGboost (distanceFromNet & angle)           |0.716       | 0.0934     |0.9066     |0.86273 |
| XGboost All features                        |0.754       | 0.0935     | 0.907     |0.86273|  
| XGboost SHAP features                       |0.646       | 0.0948     |     0.905 |0.862425|
| XGboost Lasso features                      |0.691       | 0.0950     |  0.905    |0.863467|
| Random baseline with uniform distribution   | 0.5        |            | 0.5021    |         |

Besides these metrics, our decision were taken also based on the four figures as quantitative metrics:

<figure style="display: block; width: 100%; margin:0; padding:0;">
<img src="/assets/milestone2/AllFigures_15.png" alt="" style="display:block;width:100%;margin:0;padding:0;">
<figcaption style="text-align:center;">Figure 5.1: Figures of Quantitative metrics </figcaption>
</figure>
 <br />

**ROC (Receiver Operating Characteristic) curve of the different classifiers:**
 <br />
We notice that he curves of XGboost (distanceFromNet & angle)  and  XGboost (All features) are very similar which means that the XGboost  classifier also relies more on the distance feature and considers it as a more important feature as compared to the angle feature. This explains why both SHAP method and Lasso method suggest having this feature for this task. 

**Shot Probability percentile vs Goal rate:**
The model that has the best correlation between the shot  Probability percentile and Goal rate is once again the one taking as input All features (except the period One).


**Cumulative propotion of goals**
This time, XGboost ran with the fatures suggested by Lasso method is giving the best results.

**Calibration:**


Thanks to this plot, we can say how close is our predicted probability of a goal, to the frequency of goals.
It is obvious then that, once again, the model with all features as input  (except period one) is the most calibrated one. 




Taking into consideration all the previous plots, we decide to select the XGBoost model that takes as input only both features distanceFromNet & angle in order to use it in the final phase; Testing. We will get then to have an interesting diversity regarding the models to be tested later (this one with very few features). 

[Link to best Model registred](-https://www.comet.ml/meriembchaaben/model-registry/xgboost-task5-model)
**Remark:**  in Task 6, we considred once again Xgboost as a condidate in the process of identifying the best model. 

## Task 6: Give it your best shot! 
### Overview
In this section we attempt several new ideas in order to achieve better accuracy, compared to previous sections. 

Some of these techniques were very successful (ex. tree-based methods) others were less successful (ex. NN-based methods).

In the next section we will outline the methods and techinques we've attempted, followed by our top 5 most notable results, and we will end with a brief discussion.


### Methods and Techniques
In order to improve our models' accuracy _(and achieve full marks)_, in this section we explore the following additional approaches:

1. [x] Data Train/Validation split using a **Time-Series Split**
2. [x] **Hyperparameter Tuning** using **Cross Validation**
3. [x] **Regularization** of model weights, to improve generalization / avoid overfitting
4. [x] Additional **Feature Selection** Techniques
   - [x] Model Weight-Based Feature Selection, using Support Vector Machines
   - [x] Recursive Feature Elimination-based Feature Selection, using Naive Bayes, Random Forrest and XGBoost Models
5. [x] Additional Models
   - [x] **Random Forest**
   - [x] **Neural Network** Models, 
   - [x] with different Loss functions
     - [x] Cross Entropy Loss
     - [x] Focal Loss
   - [x] with different Learning Rate policies
     - [x] One-Cycle Policy
     - [x] Stochastic Gradient Descent with Warm Restarts (SGDR) Learning Rate Policy
6. [x] Additional **Accuracy Metrics**
   - [x] **F1 Score**, Macro average, to emphasize minority class predictive accuracy
   - [x] **Brier Score**, to measure accuracy of predicted probabilities 

### Summary of Results (Top 5)

|Classifier                                          |AUC of ROC (+) |F1 (Macro Average) (+) |Brier Score (-) |Comet link                                                                  |
|----------------------------------------------------|----------|------------------|-----------|----------------------------------------------------------------------------|
|Random Forest                                       |0.75     |0.53             |**0.09**      |[Details](https://www.comet.ml/meriembchaaben/ift6758/561f6ad677da470986f66c916519e9cf)|
|Feature Selection + XGBoost + Regularization        |0.77     |0.61             |0.17      |[Details](https://www.comet.ml/meriembchaaben/ift6758/6b56e4f49b6740548f83d6a16c13dc6d)|
|**XGBoost + Regularization + Grid Search [BEST MODEL]**              |**0.77**     |**0.61**             |0.17      |[Details](https://www.comet.ml/meriembchaaben/ift6758/6cc6d08bce9844af9141e87129e9a78f)|
|Neural Network (cross entropy loss, early stopping) |0.54     |0.55             |0.09      |[Details](https://www.comet.ml/meriembchaaben/ift6758/3533c1ad47bf4d53b53edccf0de74f1e)|
|Neural Network (focal loss, gamma=2, early stopping)|0.55     |0.56             |0.10      |[Details](https://www.comet.ml/meriembchaaben/ift6758/dd864f6af8094e1ab1c19fbcb4115278)|



<!-- |Neural Network (focal loss, gamma=2, no early stopping)|0.56      |0.558             |0.159      |[Details](https://www.comet.ml/meriembchaaben/ift6758/9128c55a6c5e402ea8f1a6e660e89994)| -->


**Table 6.1:** Summary of results, validation set, top 5 best models. Top 1 Best (and final) model and best results in **bold**.

**Note**: For accuracy metrics, (+) means higer is better, (-) means lower is better.

### Our Best Model
Our best model (ROC AUC=0.774) was **XGBoost** `XGBClassifier` with :
1. No Explicit Feature Selection
2. `L1`, `L2`, and tree pruning **regularization**, 
3. **Hyperparameter Tuning** using **Cross Validation** (n=5) on a Time Series data split (`TimeSeriesSplit`) (i.e. using 5 equal-length, consecutive, non-overlappping validation sets).

Our preprocessing pipeline included:
1. Categorical Encoding, using `OrdinalEconder`
2. Missing Value Imputation (`median` for numerical, `"Missing Value"` category for categorical data)
3. Standardization, using `StandardScaler`.
To accomplish these steps we took advantage of the `sklearn` `Pipeline` functionality.
 
We selected this model, as our Best Model, due to its highest AUC ROC score among all its peers.

### Summary of Results
The best performing family of models was the Tree-based family of models.

Extreme Gradient Boosted Trees dominated across all metrics with 0.774 ROC AUC, 0.612 F1 (macro), 0.173 Brier score and were closly followed by Random Forest models with 0.748 ROC AUC, 0.525 F1 (macro), 0.091 Brier Score.

Perhaps surprisingly, Neural-Network (NN) based models were not able to fit to the data as well as Tree-based models did. 
Our best NN model achieved 0.546 ROC AUC, 0.558 F1 (macro) and 0.106 Brier score, which is worse than our best XGBoost model, with the exception of Brier scores, where lower is better.


### Detailed Comaprison of Results _(with figures)_

In this sub-section we will perform a brief quanititaive comparison of the Top 5 models. 

![ROC Curve](/assets/milestone2/q6-roc.png)
**Fig. 6.1:** Plot of the Receiver Operating Characteristic (ROC) curve for our top 5 models.

Our selected model, XGBoost without Feature Selection has the highest area under the ROC curve (AUC ROC). 

![Goal Rate Percentile](/assets/milestone2/6-goal_rate_percentile_1.png)
**Fig. 6.2:** Plot of Shot Probability percentile vs Goal rate for our top 5 models.

Although the XGBoost model has the best AUC ROC, it does not have the best correlation between the predicted Shot Probability percentile and Goal rate. Instead the Random Forrest Model shines here. 

This is due to the fact that XGBoost was trained using Log Loss, which pushes the prediction values towards 0 or 1, whereas the Random Forest model was trained using Sample Accuracy, which in our case is equivalent to the Jaccard similarity coefficient score, and would not have this same effect to the same extent.

We also observe a "spiky" behavior in the plot above. This tells us that the models illustrating this behavior do not output a continuous set of probabilities, instead their output probabilites are concetrated around ceertain levels (0.23, 0.5. 0.8). 
This phenomena too, could be explained by the the difference in optimization loss function discussed above.

![Goal Rate Percentile, Cumulative](/assets/milestone2/q6-goal_rate_percentile_2.png)
**Fig. 6.3:** Plot of Cumulative Shot Probability percentile vs Goal rate for our top 5 models.

This plot is simialr to the simple Shot Probability percentile vs Goal rate, illustrated above, and our results are also in line.

![Calibration Diagram](/assets/milestone2/6-calibration_diagram.png)
**Fig. 6.4:** Calibration Diagram plot for our top 5 models.

This plot describes the correlation between the Mean Predicted Probability for a set of samples and the Fraction of Positive labels in that sample set. In other words this plot tells us how close is our predicted probability of a goal, to the frequency of goals.

The Random Forrest model is the most well-calibrated model among our top 5 best perfroming models (in terms of ROC AUC). This insight was also highlighted when looking at the **Brier score**, _(see Table 1)_, as the Random Forest classifier had the lowest Brier Score Loss among its peers.

![Precision Recall Curve](/assets/milestone2/6-precision_recall_curve.png)
**Fig. 6.5:** Bonus plot: Precision-Recall Curves for our top 5 models.

This plot illustrated the precision-recall trade-off inherent to all classifiers. 
The higher the area under the curve (PR AUC) the better the classifier.

Interestingly we note how the Random Classifer outperformes Neural Network classifiers in this plot.

### Further Discussion of Unsucessful Models
As discussed in the Overview section, we have attempted a plethora of additional experiments which we do not have the chance to describe in detail.

Some of these experiments are:
- performing Feature Selection using Recursive Feature Elimination, based on Naive Bayes, Random Forest, and XGBoost models,
- leveraging several types of Feature Encodings (ex. One-Hot Encoding, Mean-Encoding, etc.),
- testing several accuracy metrics for Hyperparameter optimization (ex. weighted F1 score, Briar score, etc.)
- using a One-Cycle Variable learning-rate Policy to train a Neural Network,
- testign several values for the gamma parameter used in Stochastic Gradient Descent with Warm Restarts,
- and many more (as outlined in the Overview section).

Although our Neural Network-based approach was outperformed by our highly-optimized XGBoost approach, we still had some interesting observations to discuss.

Our Neural Network had the following architecture:

```
TabularModel(
  (embeds): ModuleList(
    (0): Embedding(18, 8)
    (1): Embedding(1180, 84)
    (2): Embedding(2, 2)
    (3): Embedding(32, 11)
    (4): Embedding(4, 3)
    (5): Embedding(8, 5)
    (6): Embedding(3, 3)
    (7): Embedding(4, 3)
    (8): Embedding(558, 55)
    (9): Embedding(3, 3)
    (10): Embedding(6, 4)
    (11): Embedding(4, 3)
    (12): Embedding(3, 3)
    (13): Embedding(3, 3)
    (14): Embedding(3, 3)
    (15): Embedding(3, 3)
    (16): Embedding(3, 3)
  )
  (emb_drop): Dropout(p=0.0, inplace=False)
  (bn_cont): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): LinBnDrop(
      (0): Linear(in_features=209, out_features=200, bias=False)
      (1): ReLU(inplace=True)
      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): LinBnDrop(
      (0): Linear(in_features=200, out_features=100, bias=False)
      (1): ReLU(inplace=True)
      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): LinBnDrop(
      (0): Linear(in_features=100, out_features=2, bias=True)
    )
  )
)
```

1. We pre-process the data by filling in missing values using the `median`, and standardizing numerical values. We one-hot encode categorical features.
2. We then pass the categorical data through `feature embeddings`; we have one embedding per categorical feature. This helps us achieve lower data dimensionality.
3. Next we concatenate the output of the feature embeddings, as well as the data from our numeric features. For this model we do not use drop out.
4. The next 2 layers in our Nerual Network are **Hidden Layers**, composed of:
   1. A fully connected linear layer
   2. A Recrifier Linear Unit (ReLU), non-linear activation
   3. A Batch-Normalization Layer
5. Naturally, the last layer is an **output layer**, with two outputs, (`p("Shoot"|x_i)`, `p("Goal"|x_i)`), where the two probabilites are constrained to sum to 1 and span the (0,1) range.

For the highest scoring NN model, we used a `Focal Loss` loss function. This specific loss function has the advantage of using a scaling factor ro mudulate the Cross Entropy loss as to assign higher improtance to hard negative examples. In the context of this NN model, those examples of focus were class "Goal" labels.

Next we proceed to explore multiple learning rates and select an optimal learning rate:
![Learning Rate](/assets/milestone2/fastai/fastai-fl-es-lro.png)
**Fig. 6.6:** Optimal Learning Rate Search

After selecting an optimal learning rate (orange dot) to initialize the experiment, we proceed to train our model using a **Stochastic Gradient Descent with Warm Restarts** (SGDR) learning rate policy. This policy is similar to the **One-Cycle** learning rate policy we also tested, with the main diference being that SGDR has many cycles -- in our case, at most 5, depending on early stopping.

![Learning Rate Schedule](/assets/milestone2/fastai/fastai-fl-es-lrs.png)
**Fig. 6.7:** Learning Rate Schedule: Stochastic Gradient Descent with Warm Restarts (SGDR) Learning Rate Policy

![Train and Validation Losses](/assets/milestone2/fastai/fastai-fl-es-loss.png)
**Fig. 6.8:** Training and Validation Losses by Training Batch

We can see based on the figures above how a new learning rate cycle causes the training loss to increase, as each new cycle has starts with a large learning rate that forces the optimizer out of local minima, in order to, hopefully, reach a better minima, and a drop in validation loss.

Finally, another interesting insight is that, for this particualr problem settig, feature selection systematically yielded either a large feature set or low model accuracy, which seems to indicate that the features present carry meaningful information. This finding is consistent with our observations from Section 5.
Perhaps, a good next-step in order to further improve the accuracy of our models, woud be creating additonal, domain-epertise-based features. 


## Task 7: Evaluate on test set

### Question 1

Figure 7.1 depicts the ROC curve for the different models on the 2019/20 regular season.

<figure> 
<img src="/assets/milestone2/7-roc-R.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 7.1: ROC (Receiver Operating Characteristic) curve - regular season</figcaption>
</figure>

Overall, the Best Model (selected from Task 6) has the best discrimination performance (since it has best AUC score) wheras the LR model trained on the angle features has the worst performance.
This could be explained by the fact that using only the angle feature is not efficient enough as, unlike the distance, the angle information only does not allow to have a good prediction of expected goals.
The other models have a comparable performance with a slight ascendancy towards the XGBoost model.

Another reason why the best model (selected in task 6) has the highest ROC AUC is due to the higher model complexity of the Gradient Boosted Tree model than the Logistic Regression model; the XGBoost family of models is able to better capture some of non-linear information the information in the data.

Furthermore, even though this model has higher complexity, we aimed to preserve its generalizability by using several regularization methods, including tree pruning, L1 and L2 regularization, and testing the suitability of its hyperparameters using cross-validation.

However, for the best model, in order to achieve very high ROC AUC and F-1 (macro) scores, we over-weighted the minority class (5:1), by setting `scale_pos_weight=5`, which resulted in a less well-calibrated model with a bias for predicting goals, i.e. shot probability predictions are less correlated with the goal rate.

This observation on the **Test Set** is **in line** with our observations on the **Validation Set**.

As a result if our goal was to accurately predict Shots vs Goals, our Best Model would be ideal, but if our goal was to accurately predict the Probability of a Goal, our XGBoost model from Task 5 would be ideal.

# comapre with results obtained ussing the validation set ...


Figures 7.2 and 7.3 shows respectively the goal rate and cumulative proportion of goals by shot probability model percentiel for regular season.
The *Best Model* has the best results since the goal rate the cumulative proportion of goals are higher when the probability is greater than 0.5 and lower otherwise.


<figure> 
<img src="/assets/milestone2/7-goal_rate_percentile_1-R.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 7.2: Goal rate by shot probability model percentile - regular season</figcaption>
</figure>

<figure> 
<img src="/assets/milestone2/7-goal_rate_percentile_2-R.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 7.3: Cumulative proportion of goals by shot probability model percentile- regular season</figcaption>
</figure>

Figure 7.4 shows clearly that XGBoost is the most well-calibrated model among all the models.

<figure> 
<img src="/assets/milestone2/7-calibration_diagram-R.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 7.4: Reliability diagram (calibration curve) - regular season</figcaption>
</figure>






### Question 2

Figure 7.5 shows the ROC curve for the different models on the 2019/20 playoffs.

<figure> 
<img src="/assets/milestone2/7-roc-P.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 7.5: ROC (Receiver Operating Characteristic) curve - playoffs</figcaption>
</figure>

The *Logistic Regression* models have a poor discrimination performance (parts of the curves are under the random ROC curve) since they have low AUC scores (**0.5** for *LR(distance)*, **0.516** for *LR(angle)* and **0.499** for *LR(distance+angle)*).
However, the *XGBoost* and *Best Model* (obtained from Task 5 and 6 respectively) have good performance.

The performance of the different model is worse on the playoff data as compared to the results of the regular season.
This could be explained by the fact that the models were trained on regular seasons.
In fact, regular season is different from playoffs.
 - During regular seasons, teams play more games with opponents from the same division.
However during playoffs, best and most performant teams play against each other from different divisions. Every team in the playoffs is good enough to be a champion.
- Player health might deteriorate between regular season and playoffs
- If a team is already winning the the regular season they might decide to give their great players less ice time during regular season, to preserve their health for the playoffs
- Player skill distribution will be different (only the top teams make it of the playoffs), probably harder to score
- Teams' game strategy might be different, regular season: scoring many points more important than winning many games, for the playoffs it's the opposite.

Therefore, to get better performance for playoffs, we should either include playoffs in the training data or fine-tune the models on playoffs data.


Similarly to regular season, the *Best Model* has the best results since the goal rate the cumulative proportion of goals are higher when the probability is greater than 0.5 and lower otherwise.

<figure> 
<img src="/assets/milestone2/7-goal_rate_percentile_1-P.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 7.6: Goal rate by shot probability model percentile - playoffs</figcaption>
</figure>



<figure> 
<img src="/assets/milestone2/7-goal_rate_percentile_2-P.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 7.7: Cumulative proportion of goals by shot probability model percentile- playoffs</figcaption>
</figure>

The XGBoost is the most well-calibrated model according to Figure 7.8.

<figure> 
<img src="/assets/milestone2/7-calibration_diagram-P.png" alt="" style="margin:auto;">
<figcaption style="text-align:center;">Figure 7.8: Reliability diagram (calibration curve) - playoffs</figcaption>
</figure>