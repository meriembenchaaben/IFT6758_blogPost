---
layout: post
title: MileStone 2
---


## Task 2: Feature Engineering I

## Task 3: Baseline Models
### Question 1

## Task 4: Feature Engineering II

## Task 5: Advanced Models
# XGBoost models 


### Question 1


In order to enhance  the performance of our models of predicting if an event is a goal or not, we used the  train-validation split procedure. The validation data enables us to fine-tune the model hyperparameters and make decisions regarding what changements we can apply to have better result. The validation set affects indirectly the model. 
In our case, we are working with very few hyperparameters, thus the size of our dataset is not very huge (20%) of the training Data.  
Link to comet: 



### Question 2

Hyperparameter tuning setup: 

we are using the "binary:hinge"  loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.

The learning rate is controlled by the ETA parameter. It defines the amount of "correction" we perform at each phase by corresponding to the shrinking of the weights associated with features after each cycle. A smaller eta strengthens our model's resistance to overfitting, therefore the lower the learning rate, the better.


Tuning the Number of Decision Trees in XGBoost:
We crun a grid search of the n estimators model parameter with scikit-learn, assessing this sequence of values (50, 150, 200, 250, 300, 350,400)We note that the  default in the XGBoost library is 100 . In order to evvaluate the results of each configuraton we use F1 score. We obtain the best results 350. 


Results: 

- Links to experiments in *comet.ml*:
    - [Link to the experiment of dataset statistics in comet.ml](https://www.comet.ml/meriembchaaben/ift6758/b65854538ffd4dddaf1378a9679e23f1)
    - [Link to experiments of Logistic Regression (distance) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/fbea37d0aa6d46a488c82cb6fe912fb0)
    - [Link to experiments of Logistic Regression (angle) in comet.ml ](https://www.comet.ml/meriembchaaben/ift6758/a0a1675d0895408faf2f476f91cf6e89)

- Links to registered models:


### Question 3


We proceed first by styudying the correlation between the features so we plot the following plot: 
<figure>
<img src="/assets/milestone2/correlation.png" alt="">
<figcaption style="text-align:center;">Figure 1.1: Features correlation</figcaption>
</figure>
We notice that "gameseconds" and "period" features  are very correlated. LasteventGameSeconds and period too. 
Thanks to such results we were able to remove redudant features.
In the next experiments we should be reducing  this redudancy by neglecting the "period" feature.

We also notice that distanceFromTheNet is the feature the most correlated with our target feature "goal".
This feature should be present in all the coming experiments.

From the  previous task, we can extract a set of important features based on fitted trees and this using the predifined libray plotting method plot_importance, we obtain the following plot: 
<figure>
<img src="/assets/milestone2/FeatureImportance_XGboost_.png" alt="">
<figcaption style="text-align:center;">Figure 1.1: Features Importance regarding XGboost</figcaption>
</figure>

Another tool we used to check what features are actually enhancing the prediction accuracy; SHAP library. 
we obtain this plot: 
<figure>
<img src="/assets/milestone2/Shap.png" alt="">
<figcaption style="text-align:center;">Figure 1.1: SHAP features selected</figcaption>
</figure>
The idea then is to run an XGboost model with only the Features pushing the prediction higher (shown in red).
Results: 


A second option was to run Lasso: 
In this case only two features are suggested to be selected: ['distanceFromNet', 'speed']. We peroform then a second XGboost with the already tuned  parameters previously but this time using these two features and a feature related to previousEvent.  



Feature Selection using Wrapper methods: 
We thought about using Feature importance scores  for feature selection and this was done by using selectFromModel class already provided by sklearn. Athreshold is provided to this method to select a set of features, in our case (wrapper method) this threshold is obtained after first training  and then evaluating an XGBoost model on the entire training dataset and test datasets respectively.

| Classifier                                  | AUC of ROC | Comet link |
|---------------------------------------------|:----------:|:----------:|
| XGboost All features (distance)             | 0.685      |            |
| Logistic Regression (angle)                 | 0.507      |            |
| Logistic Regression (angle+distance)        | 0.685      |            |
| Random baseline with uniform distribution   | 0.5        |            |


## Task 6: Give it your best shot! 

## Task 7: Evaluate on test set

